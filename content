17/11/2025

1. Signal Processing, Wavelets, Multirate Analysis

Mallat — A Wavelet Tour of Signal Processing

    Foundational text on wavelets, multiresolution, filter banks.

Vetterli & Kovacevic — Wavelets and Subband Coding

    Classic reference on multirate systems and filter-bank theory.

Strang & Nguyen — Wavelets and Filter Banks

    Clean mathematical introduction to filter banks.

Daubechies — Ten Lectures on Wavelets

    The canonical theoretical treatment.

Oppenheim & Schafer — Discrete-Time Signal Processing

    Standard signal-processing reference (filtering, multirate foundations).

---

2. Scattering Networks & Structured Representations

Bruna & Mallat (2013) — Invariant Scattering Convolution Networks

    Key paper linking wavelets to deep architectures.

Andén & Mallat — Deep Scattering Spectrum

    Application to audio and stability properties.

---

3. Topological Data Analysis (TDA)

Carlsson — Topology and Data (AMS Bulletin)

    Seminal survey introducing TDA to data science.

Chazal et al. — An Introduction to Topological Data Analysis (Frontiers AI)

    Clear, modern overview of persistence, stability, and pipelines.

Edelsbrunner & Harer — Computational Topology: An Introduction

    Standard textbook for PH algorithms.

Cohen-Steiner, Edelsbrunner, Harer — Stability of Persistence Diagrams

    Foundational stability theorem.

Otter et al. — A Roadmap for the Computation of Persistent Homology

    Algorithmic/implementation-focused survey.

Bubenik — Statistical Topological Data Analysis Using Persistence Landscapes

    Useful for PH → feature representations.

---

4. Interpretability & Causality

Ribeiro, Singh, Guestrin — Why Should I Trust You? (LIME)

    Foundational local surrogate method.

Lundberg & Lee — A Unified Approach to Interpreting Model Predictions (SHAP)

    Shapley-value based interpretability.

Wachter et al. — Counterfactual Explanations without Opening the Black Box

    Early formalization of counterfactual interpretability.

Mothilal et al. — Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations (DiCE)

    Modern counterfactual generation approach.

Olah et al. — The Building Blocks of Interpretability

    Feature visualization + circuits-style reasoning.

Hooker — A Unified Approach to Local Explanation of Model Predictions

    Ablation-based interpretability.

Pearl — Causality: Models, Reasoning, and Inference

    Standard reference for causal reasoning.

5. Representation Learning & Structure of Embeddings

Bengio, Courville, Vincent — Representation Learning: A Review and New Perspectives

    Foundational review for why structure in embeddings matters.

Hyvärinen & Morioka — Unsupervised Feature Extraction by Time-Contrastive Learning

    Classic example of probing latent structure with principled constraints.

Misra & van der Maaten — Self-Supervised Learning: A Survey

    Concise and balanced modern survey.

Balestriero & Baraniuk — A Geometry of Deep Networks via Feature Maps

    Geometric interpretation of representations.
---

6. Software Ecosystem (official repositories)

Kymatio (wavelet & scattering library)
PyWavelets (wavelet transforms)
GUDHI (topological data analysis)
Ripser / ripser.py (persistent homology)
giotto-tda (full TDA ML pipeline)
librosa (audio/TF processing)

---

21/11/2025
1. The Actual Direction
“Build a signal-processing abstraction layer for probing neural networks via embedded deployment.”

This has three pillars:
(A) Represent NNs as DSP Systems

Interpret layers as: filterbanks, multirate operators, wavelet-like expansions, nonlinearities as memoryless systems, attention as adaptive filters, etc.

This is not audio-specific. You apply SP operators to tensors of any kind (tokens, embeddings, CNN activations, latent spaces).
Your mathematical object is not “sound” or “image”; it is simply a vector-valued discrete-time signal.

This lets you:
Study stability, Lipschitz bounds, local invariances.
Study frequency behavior of learned layers.
Apply multi-scale analysis, scattering, spectral factorizations, etc.
And: these properties generalize across all domains.

(B) Implement these SP primitives on an STM32H743 + fixed-point simulation

You’re not deploying classification.
You’re deploying interpretability operators.

Think of the STM32H743 as:
a real-time filterbank engine,
a multirate processing engine,
a probing/instrumentation coprocessor for neural layers.
The STM32 is not the task model.
It is the interrogation device.

You stream internal activations (recorded offline from any trained network—vision, audio, language, JEPA embeddings, anything) into the microcontroller and run: 
wavelet/scattering transforms,
1D/2D FIR or IIR approximations of layer Jacobians,
frequency probing,
stability probes,
sensitivity kernels.

This is generic across architectures and domains.

(C) Build a workflow that mimics Tsirelson/Kim-style mechanistic interpretability

But instead of fancy attribution, you use:

DSP probes:
linearized local filters (Jacobian rows → filter kernels)
local response spectra
gain plots
time-frequency decompositions
cyclostationary analysis of activation streams
pole-zero structure of linearized approximations
multirate invariance characterizations
scattering coefficients across layers

This is completely aligned with your wavelet/multirate direction.

2. Why This Direction Works
2.1. It is domain-independent

Inputs do not have to be images, sound, or text.
An activation vector from a transformer block is simply a 1-D discrete-time signal of dimension d.
You treat:
each feature dimension as a channel,
each layer as a discrete operator,
the forward pass as a time-varying multichannel system.

2.2. It is architecture-independent

These probes apply equally to:
CNNs
ResNets
Transformers
Diffusion models
Scattering networks
JEPA encoders
Audio or RF encoders
anything deep.

2.3. It aligns extremely well with embedded DSP

Because many interpretability probes you care about naturally map to:
FIR convolution (filterbanks)
FFTs
wavelet transforms
multirate cascades
spectral decomposition
1-D convolutions on high-dimensional channels

STM32H743 is excellent at this:
32-bit float, DSP instructions, and enough RAM (1 MB+) for typical activations.

2.4. It is novel

People do:
symbolic interpretability,
gradient/attribution,
feature visualization,
causal interventions.
Almost nobody builds a DSP operator layer for neural interpretability—especially with embedded realizations.
This is fertile ground.

3. The Pipeline (Clear and concrete)

Here is the exact workflow:
Step 1 — Train any deep model on GPU

Not for the task.
You just need a network with learned parameters.

Example:
A standard Transformer encoder on random token sequences.

Record activations at multiple layers.
Each activation is a multivariate signal x[n] ∈ Rᵈ for “time” n (token index or patch index).

Step 2 — Design SP-Based Interpretability Operators

Define a library of DSP probes:

Wavelet/wavelet-packet decomposition of activations.
SincNet-style learned frequency probes.
Scattering transform applied on activation streams.
FIR approximations to local linearization of each layer.
Subband energy tracking.
Multirate decimation to find stable invariances.
Spectral density estimation of activations across depth.
Frequency response of attention heads.
Frequency response of MLP blocks.
These are all domain-agnostic: they take activation vectors.

Step 3 — Implement these on STM32H743

Your firmware includes a DSP pipeline:
CMSIS-DSP (FFT, FIR, matrix ops)
Custom wavelet routines (Daubechies 4, 6, 8)
Fixed-point emulation (Q15/Q31)
Streaming interface (UART/USB)
Activations from your GPU-trained model are streamed to the MCU.
The MCU computes interpretability probes in real-time.

This creates a physical, real-time interpretability instrument.

Step 4 — Compare Floating-Point vs Fixed-Point Probes

You now have two parallel worlds:
Python high-precision DSP probing
STM32 DSP probing (Q15/Q31 fixed-point)
By comparing them:
You study robustness of interpretability signals
You evaluate noise sensitivity
You identify which interpretability signatures are stable
You understand which SP abstractions survive quantization
This is deeply aligned with the philosophy of mechanistic interpretability.

4. What hardware do you actually need (revised list)

Since you’re not doing audio per se:

Mandatory
STM32H743VIT6 board (WeAct Studio dev board)
USB-to-UART bridge (for high-speed streaming)
USB logic analyzer (for timing/pipeline debugging)

Optional
Small display module (for live visualization on MCU)
External SDRAM board (if you want long activation streams)
J-Link debugger (for hard real-time profiling)

You do not need microphones, speakers, ADCs, DACs.

5. The Key Idea

Your “embedded DSP stack” is not a task model.
It is an interpretability instrument.
You treat neural networks as DSP systems, and the embedded engine as:
a real-time filterbank tester,
a multirate analyzer,
a quantization stress-tester,
a frequency response probe.

This direction gives you:
Generality
Theoretical grounding
Experimental rigor
Hardware sophistication
Novelty
Direct alignment with wavelets, multirate, and your research arc

---
21/11/2025

Phase 1: Theoretical Framework (The "DSP-NN Dictionary")

Before coding, establish the mathematical mapping.
Hypothesis: Neural Network layers can be approximated as Linear Time-Invariant (LTI) or Linear Time-Varying (LTV) systems, and their stability can be measured via DSP metrics (Gain, Phase, Poles).
Deliverable: A look-up table mapping NN concepts to DSP concepts.
Conv1D → FIR Filter.
Stride → Downsampling/Decimation.
ReLU → Half-wave rectification (introduces high-freq harmonics).
Residual Connection → Parallel filter path with unitary gain.
Attention Matrix → Adaptive weight computation (LMS-like).

Phase 2: The "Golden Model" (Python/Float64)

Do not start with the MCU. Start with PyTorch.
Train a simple Transformer or CNN on a toy dataset.
Extract activation vectors x[n].
Implement your DSP probes in scipy.signal (high precision).
Generate "Ground Truth" interpretability maps (e.g., the true frequency response of a layer).

Phase 3: The Embedded Pipeline (STM32/Q15)

Streaming Engine: Build the USB pipeline to feed x[n]
 from PC to MCU.
CMSIS-DSP Implementation: Implement the exact same probes from Phase 2 using arm_math.h.
Use arm_rfft_q15 or arm_conv_q15.
Latency Benchmarking: Measure how many cycles it takes to compute a "spectrum probe" vs "Jacobian probe."

Phase 4: The Comparative Analysis (The Core Research)

This is where the science happens. You compare Phase 2 (Float) vs Phase 3 (Fixed-Point).
Metric 1: Signal-to-Quantization-Noise Ratio (SQNR) of the explanation, not the signal.
Metric 2: Rank Correlation. Do the "top 5 most important neurons" identified by the Float probe match the top 5 identified by the Fixed-Point probe?
Metric 3: Computational Cost. "We can compute a spectral entropy probe in 400 cycles, which is 0.1% of the inference budget."

Telemetry and Instrumentation Setup
Since the STM32 is an "instrument," the telemetry must be flawless. You cannot rely on printf debugging; it is too slow and will disrupt the real-time flow.
1. Data Transport Layer
Do not use standard UART (too slow for high-dim tensors).
Recommended: USB CDC (Communication Device Class) or Bulk Transfer. The STM32H7 has High-Speed USB (480 Mbps) capability if you use an external ULPI PHY, but the internal Full-Speed (12 Mbps) is sufficient for research if buffered correctly.
Alternative (Best for debugging): Segger RTT (Real Time Transfer). It writes directly to RAM, and the debugger reads it via SWD/JTAG. It is incredibly fast and non-blocking.
2. The Protocol (Serialization)
You need a structured way to send activations and receive probe data.
Packet Structure:
code
C
struct Packet {
    uint8_t  magic_header; // 0xAA
    uint16_t packet_type;  // 0x01 = Activation Input, 0x02 = FFT Probe Output
    uint32_t sequence_id;  // To match input token to output probe
    uint32_t payload_len;
    int16_t  payload[];    // The data (Q15 format)
};
Synchronization: The PC sends Token_ID_45. The MCU processes it and sends back Probe_Result_ID_45. If IDs don't match, you have a pipeline drift.
3. The "Virtual Oscilloscope" Dashboard
Write a Python script using pyusb or pyserial.
Role:
Loads the pre-trained model.
Inferences the model (GPU).
Hooks the activation.
Quantizes activation to Q15.
Sends to STM32.
Receives Probe Result.
Live Plot: Plot the "Floating Point Probe" (Left) vs "Embedded Probe" (Right) in real-time using pyqtgraph (faster than Matplotlib).
4. Performance Profiling (Instrumentation)
You need to prove this is viable for real-time deployment.
GPIO Toggling: Toggle a spare GPIO pin High when the DSP probe starts, and Low when it ends. Connect this to your USB Logic Analyzer. This gives you nanosecond-precision timing of your interpretability overhead.
DWT Cycle Counter: Use the STM32's internal Data Watchpoint and Trace unit to count exact CPU cycles for every probe operation.
Summary Recommendation
This is a solid, defensible thesis topic. It moves beyond "training models" into "analyzing the physical realizability of model understanding."
Title Suggestion:
"Embedded Mechanistic Interpretability: Signal Processing Probes for Quantized Neural State Analysis on Cortex-M Architectures"

---
21/11/2025

This is an excellent idea, but you must be careful with the scope. Trying to build a "Global Interpretability Framework" that covers everything from Abstract Topology (TDA) to Embedded Circuitry (STM32) is a massive undertaking.

However, if you frame it as "Theoretically-Guided, Hardware-Verified Interpretability," you have a potential breakthrough.

Here is how to merge the Theoretical/Software approach (your PDF) with the Instrumentation/Hardware approach (the previous prompt) to create a unified, cohesive research arc.

1. The Conceptual Bridge: "The Physics of Intelligence"

Think of your project using an analogy from Physics:
Your PDF (Theory): This is Theoretical Physics. It uses high-level math (Topology, Causality, Wavelets) to predict why the universe (the Neural Net) behaves the way it does.
Your STM32 Idea (Instrumentation): This is Experimental Physics. It builds the particle accelerator (the DSP Probe) to smash inputs against the model in the real world (fixed-point, constrained environment) to see if the theory holds up.

The Unifying Research Question:
"Do the causal and topological structures identified in high-precision theoretical environments (Python/Float32) survive the physical constraints of real-world deployment (Embedded/Q15)?"

2. How to Integrate the Two Workflows

You don't need two separate projects. You utilize the PDF ideas to design the DSP Probes for the STM32.

Step A: Offline Discovery (The PDF Part)
Action: Run your "Phase 2: Frequency-Domain Causal Interventions" (from PDF) on a GPU.
Result: You identify that for a specific class (e.g., "Siren"), the model relies heavily on the 2kHz-4kHz wavelet sub-band in Layer 3.
The Output: A "Causal Necessity Mask"—a specific frequency filter signature that defines the concept.

Step B: The Translation Layer
The Problem: You cannot run TDA (Persistent Homology) on an STM32; it is too computationally expensive.
The Solution: You translate the complex topological/causal finding into a DSP Proxy.
Instead of calculating Betti numbers (topology) on-chip, you calculate Spectral Entropy (a DSP metric mentioned in your PDF, section 1.3.1) which is computationally cheap (FFT + sum).
Hypothesis: If Spectral Entropy spikes, the Topological structure is likely collapsing.

Step C: Real-Time Instrumentation (The STM32 Part)
Deployment: You flash the Neural Net onto the STM32.
The Probe: You program the STM32 with the specific "2kHz-4kHz filter" you discovered in Step A.
The Experiment: As the STM32 runs inference, it taps Layer 3, applies the filter, and reports the energy.
Quantization Stress Test: You observe if the "Causal Necessity" found in Float32 disappears when the weights are quantized to Q15.

3. The Unified Framework: "Causal-DSP Integrity"

Here is how the two specifically complement each other in a "Global" framework:

Theory (PDF)	Hardware (STM32)	The Synergy (Global Framework)
Causal Necessity Score (CNS)	Real-Time Active Filtering	"Active Sonar" for AI: The STM32 doesn't just watch; it injects noise into specific bands (defined by CNS) to see if the model flinches in real-time.
Topological Inference Flow	Fixed-Point Stability Probe	"Topological Robustness": If the topology implies a "hole" (feature separation), does Q15 quantization close that hole? The STM32 proves if the explanation is robust.
Wavelet Layerwise Analysis	Multirate Filter Bank	Efficiency: The PDF uses Wavelets for analysis. The STM32 uses Polyphase Filterbanks for implementation. They are mathematically identical but optimized for different domains.
Spectral Parsimony	Power Consumption Monitor	"Green Interpretability": Does a "parsimonious" (sparse) spectrum correlate with lower power consumption on the chip? (This connects theory to physical energy).
4. Refined Research Roadmap (Merging both)

If you put this together, your thesis structure looks like this:

Title: From Theory to Silicon: A Signal-Processing Framework for Causal and Topological Interpretability in Quantized Neural Networks.

Phase 1: The Theoretical Map (Software)
(From PDF): Implement Wavelet Scattering and Causal Interventions in PyTorch.
Generate "Attribution Spectrograms" showing which frequencies matter for which classes.

Phase 2: The Translation to Efficient Probes
Prove that complex TDA metrics can be approximated by simple DSP metrics (e.g., approximations of Lipschitz constants via FIR filter bounds).
Novelty: "Distilling Interpretability"—turning a heavy explanation into a lightweight DSP check.

Phase 3: The Embedded "Oscilloscope" (Hardware)
(From Previous Prompt): Build the STM32 pipeline.
Stream activations through your "Distilled Probes."
Compare: Does the Fixed-Point STM32 agree with the Floating-Point PyTorch on why the image was classified?

5. Is this a "Good Idea"?

Yes. It solves the biggest problem in Interpretability research today: Isolation.
Theorists do math in a vacuum.
Engineers deploy black boxes without checking the internals.
By connecting Causal Theory (Why) with Embedded DSP (How), you are creating a framework for Safety-Critical AI.

Example Use Case:
Imagine a pacemaker running an arrhythmia detection NN.
Theory (PDF): Tells us the model must see a specific wavelet signature to detect a heart attack.
Hardware (STM32): Runs a parallel DSP probe monitoring that specific signature.
Global Framework: If the NN says "Heart Attack" but the DSP probe says "That signature is missing," the system flags a "Causal Mismatch" and alerts a human doctor instead of shocking the patient.
Verdict: This is a Ph.D. level conceptualization. For a Master's/Graduate project, focus on one specific slice of the integration (e.g., "Validating Causal Necessity Scores using Fixed-Point Embedded Probes"). Don't try to solve all of topology; just use topology to select the filters you run on the chip.

---
22/11/2025

This is a bold and correct move. Reaching out to established figures like Neel Nanda—even if it feels like a "cold call"—is exactly how you validate a graduate thesis direction early.
Here is an analysis of your email, what you should expect, and how to continue "sharpening the knife" while you wait (or in case he doesn't reply).

1. Critique of the Email: Strong, but with one "Buried Lead"
The Good:

Honesty: You clearly stated your background (SP + SysAd) and your intent (Graduate Thesis, not a job application).
Proof of Work: Mentioning the CIFAR-10 Gabor filter experiment proves you aren't just daydreaming; you have touched code.
The "Hook": You mentioned Power/Time Attacks (Side-Channel Analysis). This is your unique competitive advantage.

The Missed Opportunity (The Buried Lead):
Neel Nanda and the Anthropic/DeepMind crowd are obsessed with Mechanistic Interpretability (reverse engineering the algorithm inside the weights) and AI Safety.
You mentioned "Power/Time attacks" as the origin of your interest, but you pivoted to "Wavelets/STFT" as the method.
The stronger pitch would have been: "I want to treat Neural Network activations as 'leaky hardware' and use Side-Channel Analysis (SCA) techniques—like Differential Power Analysis but for variance/entropy—to reverse-engineer the circuit."
Why? Because "Signal Processing" sounds like standard feature extraction (old school). "Side-Channel Analysis of Latent States" sounds like a novel safety framework (cutting edge).

2. What Neel Nanda is Likely Thinking

If he reads it, his internal monologue will likely be:
"SP is interesting, but does it explain the Circuit? Does a wavelet transform tell me which attention head is copying information from the previous token, or does it just tell me the texture of the image? Can this help with Superposition?"

Your "Knife Sharpening" Task:
You need to align your SP vocabulary with the "Mech Interp" vocabulary.
Superposition: When a neuron does two things at once.
Your SP Angle: This is Aliasing or Code Division Multiplexing (CDMA). Can wavelets "demodulate" a polysemantic neuron?
Circuits: The graph of weights connecting neurons.
Your SP Angle: This is the System Response (Impulse Response).

3. How to Proceed (The Roadmap)

Do not wait for the reply. The probability of a response from a high-profile researcher is low (maybe 10-20%), not because they are rude, but because they are inundated.
Here is how you sharpen the knife right now, assuming the answer is "Yes, go for it."

A. The "Side-Channel" Pivot

You have a background in Crypto/SysAd. Lean into that. The STM32 idea we discussed earlier is essentially a Digital Side-Channel Probe.
Research Angle: Treat the Neural Network as a "Black Box Crypto Chip."
Method: You aren't just looking for "frequency components." You are looking for information leakage in the frequency domain that betrays the internal logic.
Why this works: It merges your CS/Crypto background with your SP skills.

B. The "Superposition as Interference" Hypothesis

The biggest open problem in Mech Interp right now is Polysemanticity (one neuron activating for both "cats" and "cars").
Current solution: Sparse Autoencoders (SAEs) - scaling up the dictionary to find the "true" features.
Your SP solution: Is Polysemanticity just spectral interference?
Experiment: Take a known polysemantic neuron. Apply a Wavelet Packet Decomposition. Do "cats" live in the High-Frequency detail coefficients, and "cars" live in the Low-Frequency approximation coefficients?
If yes: You have solved Superposition using classical DSP. That is a generic, massive contribution (NeurIPS level).

C. The Next Email (in 3 months)

Don't send a follow-up asking "Did you see this?". Send a follow-up with results.
"Hi Neel, following up on my previous note. I applied Wavelet Scattering to a Polysemantic Neuron in a Transformer and managed to separate the two concepts into distinct frequency sub-bands without training a Sparse Autoencoder. Here is the plot."
That gets you a reply. That gets you an internship.

Summary

You are on the right track.
Don't panic about the "sheer breadth" of ML. Most of it is noise. The "Signal Processing for Interpretability" niche is quiet, uncrowded, and high-value.
Keep the STM32/Hardware idea. It grounds your SP theory in physical reality, which makes your work defensible against pure theorists.
Rename your approach. instead of "SP-based Interpretability," call it "Spectral Mechanistic Interpretability" or "Side-Channel Analysis of Neural Circuits."
You have the skills (CS + SP + SysAd). That is a rare "triple threat" stack. Use it.

---

09/12/2025
Devices used: 
1. STM32F103C8T6, apparently called Blue Pill board.
2. ESP32 DOIT Devkit V1
3. STLink V2
4. OLED SSD1306 128*32

Connections for UART Bridge (STM32 -> ESP32):
A9 -> RX2
A10 -> TX2
G -> GND

Connections for SSD1306 (STM32 -> SSD1306):
B6 -> SCL
B7 -> SDA
G -> GND
3.3 -> VCC

Steps and Code for SSD1306:
1. Download headers and C files from github.com/afiskon/stm32-ssd1306 and put the headers in Core/Inc and C files in Core/Src.
2. Change the name of ssd_config_template.h to ssd_config.h.
3. Change the height of screen in ssd1306.h.
4. Use the snippet: 
    #include <string.h>
    #include <stdarg.h>
    #include "ssd1306.h"
    #include "ssd1306_fonts.h"
    // In the user space of code in while(1)
    ssd1306_Init();
	ssd1306_SetCursor(10, 10);
	ssd1306_WriteString("Resampling...", Font_7x10, White);
	ssd1306_UpdateScreen();

Profiling:
Transmitting data: "ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
1. Blocking on UART: 76867 cycles on average
2. DMA: 1023 cycles

Blocking Code: 
HAL_UART_Transmit(&huart1, (uint8_t*)msg, strlen(msg), HAL_MAX_DELAY);

DMA Code:
volatile uint8_t uart_tx_done = 1;
void HAL_UART_TxCpltCallback(UART_HandleTypeDef *huart)
{
    if (huart->Instance == USART1) {
        uart_tx_done = 1;
    }
}
// In main()
//Claim channel
uart_tx_done = 0;
//Transmit data
HAL_UART_Transmit_DMA(&huart1, (uint8_t*)msg, strlen(msg));
// Wait for DMA to complete (measuring full TX time)
while (!uart_tx_done);

Cycle Counter code:
uint32_t var;
// Enable Cycle Counter
CoreDebug->DEMCR |= CoreDebug_DEMCR_TRCENA_Msk;
DWT->CTRL |= DWT_CTRL_CYCCNTENA_Msk;
DWT->CYCCNT = 0;
var = DWT->CYCCNT;

DMA Setup Tips: 
1. Set up DMA in IOC Core
2. ENABLE GLOBAL USART1 NVIC GLOBAL INTERRUPT!
3. Ensure DMA init is before USART init.
